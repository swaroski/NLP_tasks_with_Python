{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5744b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\"The sun sets behind the mountains, casting a warm orange glow.\",\n",
    "            \"In a distant galaxy, a new civilization is born, full of hope and curiosity.\",\n",
    "            \"Amidst the bustling city, a street musician plays a melancholic tune on his guitar.\",\n",
    "            \"The aroma of freshly baked bread fills the cozy bakery, enticing passersby.\",\n",
    "            \"A lone wolf prowls through the dense forest, its eyes gleaming in the moonlight.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c4e502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'sun', 'sets', 'behind', 'the', 'mountains', ',', 'casting', 'a', 'warm', 'orange', 'glow', '.'], ['In', 'a', 'distant', 'galaxy', ',', 'a', 'new', 'civilization', 'is', 'born', ',', 'full', 'of', 'hope', 'and', 'curiosity', '.'], ['Amidst', 'the', 'bustling', 'city', ',', 'a', 'street', 'musician', 'plays', 'a', 'melancholic', 'tune', 'on', 'his', 'guitar', '.'], ['The', 'aroma', 'of', 'freshly', 'baked', 'bread', 'fills', 'the', 'cozy', 'bakery', ',', 'enticing', 'passersby', '.'], ['A', 'lone', 'wolf', 'prowls', 'through', 'the', 'dense', 'forest', ',', 'its', 'eyes', 'gleaming', 'in', 'the', 'moonlight', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing text into bags of words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb67d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'sun', 'sets', 'behind', 'the', 'mountains', 'casting', 'a', 'warm', 'orange', 'glow'], ['In', 'a', 'distant', 'galaxy', 'a', 'new', 'civilization', 'is', 'born', 'full', 'of', 'hope', 'and', 'curiosity'], ['Amidst', 'the', 'bustling', 'city', 'a', 'street', 'musician', 'plays', 'a', 'melancholic', 'tune', 'on', 'his', 'guitar'], ['The', 'aroma', 'of', 'freshly', 'baked', 'bread', 'fills', 'the', 'cozy', 'bakery', 'enticing', 'passersby'], ['A', 'lone', 'wolf', 'prowls', 'through', 'the', 'dense', 'forest', 'its', 'eyes', 'gleaming', 'in', 'the', 'moonlight']]\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation\n",
    "import re\n",
    "import string\n",
    "regex = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
    "\n",
    "# Initialize an empty list to store tokenized documents without punctuation\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "# Loop through each tokenized review in the tokenized_docs list\n",
    "for review in tokenized_docs:\n",
    "    # Initialize an empty list to store tokens in the current review without punctuation\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        # Remove punctuation from the token using the regex pattern\n",
    "        new_token = regex.sub(u'', token)\n",
    "        # Check if the new token is not an empty string\n",
    "        if not new_token == u'':\n",
    "            # Append the cleaned token to the new_review list\n",
    "            new_review.append(new_token) \n",
    "    # Append the new_review (cleaned review) to the tokenized_docs_no_punctuation list\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5554255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'sun', 'sets', 'behind', 'mountains', 'casting', 'warm', 'orange', 'glow'], ['In', 'distant', 'galaxy', 'new', 'civilization', 'born', 'full', 'hope', 'curiosity'], ['Amidst', 'bustling', 'city', 'street', 'musician', 'plays', 'melancholic', 'tune', 'guitar'], ['The', 'aroma', 'freshly', 'baked', 'bread', 'fills', 'cozy', 'bakery', 'enticing', 'passersby'], ['A', 'lone', 'wolf', 'prowls', 'dense', 'forest', 'eyes', 'gleaming', 'moonlight']]\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the text of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words(\"english\"):\n",
    "            new_term_vector.append(word)\n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "    \n",
    "print(tokenized_docs_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29df8486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'sun', 'sets', 'behind', 'mountains', 'casting', 'warm', 'orange', 'glow'], ['In', 'distant', 'galaxy', 'new', 'civilization', 'born', 'full', 'hope', 'curiosity'], ['Amidst', 'bustling', 'city', 'street', 'musician', 'plays', 'melancholic', 'tune', 'guitar'], ['The', 'aroma', 'freshly', 'baked', 'bread', 'fills', 'cozy', 'bakery', 'enticing', 'passersby'], ['A', 'lone', 'wolf', 'prowls', 'dense', 'forest', 'eyes', 'gleaming', 'moonlight']]\n"
     ]
    }
   ],
   "source": [
    "#Stemming and lemmatizing\n",
    "'''\n",
    "Porter Stemmer (PorterStemmer): Porter stemming is an algorithm that attempts to remove suffixes (and sometimes prefixes) \n",
    "from words in order to obtain their root form. It is a simple and widely used stemming algorithm. \n",
    "For example, it would reduce words like \"running,\" \"runner,\" and \"ran\" to the root form \"run.\" \n",
    "\n",
    "However, it may not always produce valid words, as it focuses on rule-based transformations rather than linguistic accuracy. \n",
    "Porter stemming is fast and efficient.\n",
    "\n",
    "Snowball Stemmer (SnowballStemmer): Snowball stemming is an extension of the Porter stemming algorithm and is available \n",
    "in multiple languages. It aims to provide more accurate stemming by applying language-specific rules. The 'english' variant \n",
    "specifically targets the English language. It's considered an improvement over the original Porter stemmer and can \n",
    "produce more linguistically valid stems.\n",
    "\n",
    "WordNet Lemmatizer (WordNetLemmatizer): Lemmatization is a different approach from stemming. Instead of reducing words to \n",
    "their root forms, lemmatization reduces words to their base or dictionary forms (lemmas). \n",
    "The WordNet Lemmatizer is based on WordNet, a lexical database of English. It can provide more accurate results \n",
    "because it considers word meanings and parts of speech. For example, it would reduce \"running,\" \"runner,\" and \"ran\" to the\n",
    "lemma \"run.\" Lemmatization often results in valid dictionary words but can be slower than stemming.\n",
    "'''\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "wordnet = WordNetLemmatizer() \n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    #print(doc)\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        #print(word)\n",
    "        final_doc.append(word)\n",
    "    preprocessed_docs.append(final_doc)\n",
    "    \n",
    "print(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437149e",
   "metadata": {},
   "source": [
    "## Basic Tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0a2637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\bhise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     C:\\Users\\bhise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\bhise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     C:\\Users\\bhise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\bhise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\bhise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('genesis')\n",
    "nltk.download('inaugural')\n",
    "nltk.download('nps_chat')\n",
    "nltk.download('webtext')\n",
    "nltk.download('treebank')\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba5bd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Sense and Sensibility by Jane Austen 1811>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327a331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
     ]
    }
   ],
   "source": [
    "sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eaa085a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call', 'me', 'Ishmael', '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdef1220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Wall Street Journal> 100676\n"
     ]
    }
   ],
   "source": [
    "print(text7, len(text7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64634eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'] 18\n"
     ]
    }
   ],
   "source": [
    "print(sent7, len(sent7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b1bc38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['firmness',\n",
       " '130',\n",
       " 'players',\n",
       " 'predict',\n",
       " 'reservations',\n",
       " '47.1',\n",
       " 'palace',\n",
       " 'providing',\n",
       " 'legal',\n",
       " '6.70']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(text7))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d7bd4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Compute the frequency distribution of words in 'text7'\n",
    "dist = FreqDist(text7)\n",
    "len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5191c678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre', 'Vinken', ',', '61', 'years', 'old', 'will', 'join', 'the', 'board']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1 = list(dist.keys())\n",
    "# Get a list of the first 10 unique words in the vocabulary\n",
    "vocab1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2d68493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist['Vinken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19d19be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['billion',\n",
       " 'company',\n",
       " 'president',\n",
       " 'because',\n",
       " 'market',\n",
       " 'million',\n",
       " 'shares',\n",
       " 'trading',\n",
       " 'program']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqwords = [w for w in vocab1 if len(w) > 5 and dist[w] > 100]\n",
    "freqwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce667c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'listed', 'lists', 'listing', 'listings']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different forms of the same \"word\"\n",
    "input1 = \"List listed lists listing listings\"\n",
    "words1 = input1.lower().split(' ')\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1f9e8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'list', 'list', 'list', 'list']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in words1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2742c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Life',\n",
       " 'in',\n",
       " 'general',\n",
       " 'is',\n",
       " 'a',\n",
       " 'slippery',\n",
       " 'slope',\n",
       " 'with',\n",
       " 'many',\n",
       " 'crests',\n",
       " 'and',\n",
       " 'troughs.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization\n",
    "text1 = \"Life in general is a slippery slope with many crests and troughs.\"\n",
    "text1.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e11fb233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Life',\n",
       " 'in',\n",
       " 'general',\n",
       " 'is',\n",
       " 'a',\n",
       " 'slippery',\n",
       " 'slope',\n",
       " 'with',\n",
       " 'many',\n",
       " 'crests',\n",
       " 'and',\n",
       " 'troughs',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d08db21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence splitting\n",
    "text12 = \"The shimmering stars illuminated the night sky, creating a mesmerizing celestial display. Wow\"\n",
    "sentences = nltk.sent_tokenize(text12)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd9e0d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The shimmering stars illuminated the night sky, creating a mesmerizing celestial display.',\n",
       " 'Wow']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e697f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
